{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Hyperparameter optimization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is based on the [Kaggle: Predicting a Biological Response](https://www.kaggle.com/c/bioresponse).  \n",
    "It is necessary to predict the biological response of molecules (column 'Activity') from their chemical composition (columns D1-D1776)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data description**  \n",
    "The data is presented in CSV format. Each line represents a molecule.\n",
    "- The first Activity column contains experimental data describing the actual biological response [0, 1];\n",
    "- The remaining columns D1-D1776 are molecular descriptors - these are calculated properties that can capture some characteristics of a molecule, such as size, shape, or composition of elements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of the task:**  \n",
    "Two models need to be trained: logistic regression and random forest. Next, you need to make a selection of hyperparameters using basic and advanced optimization methods.\n",
    "It is important to use four methods (GridSeachCV, RandomizedSearchCV, Hyperopt, Optuna) at least once, the maximum number of iterations should not exceed 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree \n",
    "from sklearn import ensemble \n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials, space_eval\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and explore data\n",
    "data = pd.read_csv('files/_train_sem.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2034\n",
       "0    1717\n",
       "Name: Activity, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of a target value\n",
    "data['Activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1776) (751, 1776) (3000,) (751,)\n"
     ]
    }
   ],
   "source": [
    "# split our data\n",
    "X = data.drop('Activity', axis=1)\n",
    "y = data['Activity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
    "\n",
    "# check the samples demension\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Part I. Simple model without optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for simple logistic regression: 0.893\n",
      "Test F1-score for simple logistic regression: 0.777\n"
     ]
    }
   ],
   "source": [
    "# build a simple model\n",
    "log_reg = linear_model.LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# training this model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "print('Train F1-score for simple logistic regression: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print('Test F1-score for simple logistic regression: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for a random forest: 0.763\n",
      "Test F1-score for a random forest: 0.737\n"
     ]
    }
   ],
   "source": [
    "# build a model\n",
    "forest = ensemble.RandomForestClassifier(random_state=42, n_estimators=50, max_depth=3)\n",
    "\n",
    "# training this model\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = forest.predict(X_train)\n",
    "print('Train F1-score for a random forest: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = forest.predict(X_test)\n",
    "print('Test F1-score for a random forest: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Part II. GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for logistic regression with GridSearchCV: 0.831\n",
      "Test F1-score for logistic regression with GridSearchCV: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "param_grid = [{'penalty': ['l2', 'none'],\n",
    "              'solver': ['lbfgs', 'sag'],\n",
    "              'C': list(np.linspace(0.01, 1, 10, dtype=float))},\n",
    "              \n",
    "              {'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "              'C': list(np.linspace(0.01, 1, 10, dtype=float))}\n",
    "]\n",
    "\n",
    "# call the optimization algorithm\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# train ths model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "print('Train F1-score for logistic regression with GridSearchCV: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print('Test F1-score for logistic regression with GridSearchCV: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for random forest with GridSearchCV: 0.980\n",
      "Test F1-score for random forest with GridSearchCV: 0.799\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(100, 250, 25)), \n",
    "    'max_depth': list(range(5, 15, 1)),\n",
    "    'min_samples_leaf': list(range(1, 6, 1)),\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# call the optimization algorithm\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=forest,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# predict and calculate results for the training\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = grid_search.predict(X_train)\n",
    "print('Train F1-score for random forest with GridSearchCV: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print('Test F1-score for random forest with GridSearchCV: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Part III. RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for logistic regression with RandomizedSearchCV: 0.831\n",
      "Test F1-score for logistic regression with RandomizedSearchCV: 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "param_grid = [{'penalty': ['l2', 'none'],\n",
    "              'solver': ['lbfgs', 'sag'],\n",
    "              'C': list(np.linspace(0.01, 1, 10, dtype=float))},\n",
    "              \n",
    "              {'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "              'C': list(np.linspace(0.01, 1, 10, dtype=float))}\n",
    "]\n",
    "\n",
    "# call the optimization algorithm\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=param_grid,\n",
    "    cv=5,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# training this model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = random_search.predict(X_train)\n",
    "print('Train F1-score for logistic regression with RandomizedSearchCV: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print('Test F1-score for logistic regression with RandomizedSearchCV: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for random forest with RandomizedSearchCV: 0.98\n",
      "Test F1-score for random forest with RandomizedSearchCV: 0.80\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(100, 250, 25)), \n",
    "    'max_depth': list(range(5, 15, 1)),\n",
    "    'min_samples_leaf': list(range(1, 6, 1)),\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# call the optimization algorithm\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=forest,\n",
    "    param_distributions=param_grid,\n",
    "    cv=5,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# training this model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = random_search.predict(X_train)\n",
    "print('Train F1-score for random forest with RandomizedSearchCV: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print('Test F1-score for random forest with RandomizedSearchCV: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Part IV. Hyperopt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random state because of a stupid undecisionable error\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:15<00:00,  3.78s/trial, best loss: -0.7799713634306372]\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "space = hp.choice('parameter_combinations', [\n",
    "        {'solver': 'saga',\n",
    "         'penalty': hp.choice('penalty', ['l1', 'l2']),\n",
    "         'C': hp.uniform('C_saga', 0.01, 1)},\n",
    "        \n",
    "        {'solver': 'lbfgs',\n",
    "        'penalty': 'l2',\n",
    "        'C': hp.uniform('C_lbfgs', 0.01, 1)}\n",
    "])\n",
    "\n",
    "\n",
    "def obj_func(params, cv=5, X=X_train, y=y_train, random_state=RANDOM_STATE):\n",
    "    \"\"\"func for finding best params with cross validarion with hyperopt\"\"\"\n",
    "    \n",
    "    # params space\n",
    "    params = {\n",
    "        'solver': params['solver'], \n",
    "        'penalty': params['penalty'], \n",
    "        'C': params['C']\n",
    "    }\n",
    "      \n",
    "    # model with set of params\n",
    "    model = linear_model.LogisticRegression(**params, class_weight='balanced', \n",
    "        random_state=random_state, max_iter=50\n",
    "    )\n",
    "      \n",
    "    # use cross validation\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "    return -score \n",
    "\n",
    "# logging results\n",
    "trials = Trials()\n",
    "\n",
    "# find best params for our space\n",
    "best = fmin(obj_func, \n",
    "          space=space, \n",
    "          algo=tpe.suggest, \n",
    "          max_evals=20, \n",
    "          trials=trials, \n",
    "          rstate=np.random.RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-score for log reg with Hyperopt: 0.851\n",
      "Test F1-score for log reg with Hyperopt: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# found the best parameters\n",
    "best_params = {\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l2', \n",
    "    'C': 0.10567819922023905\n",
    "}\n",
    "    \n",
    "# build a model with best params\n",
    "hyperopt_lr = linear_model.LogisticRegression(\n",
    "    **best_params, class_weight='balanced',random_state=RANDOM_STATE, max_iter=50\n",
    ")\n",
    "\n",
    "# training this model\n",
    "hyperopt_lr.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = hyperopt_lr.predict(X_train)\n",
    "print('Train F1-score for log reg with Hyperopt: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = hyperopt_lr.predict(X_test)\n",
    "print('Test F1-score for log reg with Hyperopt: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:24<00:00,  4.21s/trial, best loss: -0.8135020917624874]\n",
      "Train F1-score for random forest with Hyperopt: 0.965\n",
      "Test F1-score for random forest with Hyperopt: 0.803\n"
     ]
    }
   ],
   "source": [
    "# set search space\n",
    "space = {'n_estimators': hp.quniform('n_estimators', 100, 250, 25),\n",
    "       'max_depth' : hp.quniform('max_depth', 5, 15, 1),\n",
    "       'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 6, 1),\n",
    "       'criterion': hp.choice('criterion', ['gini', 'entropy'])}\n",
    "\n",
    "# fix random seed\n",
    "random_state = 42\n",
    "\n",
    "def hyperopt_rf(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    \"\"\"func for finding best params with cross validarion with hyperopt\"\"\"\n",
    "    \n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth']), \n",
    "              'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "              'criterion': params['criterion']}\n",
    "  \n",
    "    # build a model with set of params\n",
    "    model = ensemble.RandomForestClassifier(\n",
    "        **params, class_weight='balanced', n_jobs=-1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # training this model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # use cross validate\n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"f1\", n_jobs=-1).mean()\n",
    "\n",
    "    return -score\n",
    "\n",
    "\n",
    "# logging results\n",
    "trials = Trials() # используется для логирования результатов\n",
    "\n",
    "# finding best params\n",
    "best = fmin(hyperopt_rf, # наша функция \n",
    "            space=space, # пространство гиперпараметров\n",
    "            algo=tpe.suggest, # алгоритм оптимизации, установлен по умолчанию, задавать необязательно\n",
    "            max_evals=20, # максимальное количество итераций\n",
    "            trials=trials, # логирование результатов\n",
    "            rstate=np.random.RandomState(42) \n",
    "            )\n",
    "\n",
    "# build a model with best params\n",
    "model = ensemble.RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    n_estimators=int(best['n_estimators']),\n",
    "    max_depth=int(best['max_depth']),\n",
    "    min_samples_leaf=int(best['min_samples_leaf']),\n",
    "    criterion=space_eval(space, best)['criterion']\n",
    ")\n",
    "\n",
    "# train this model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the training\n",
    "y_train_pred = model.predict(X_train)\n",
    "print('Train F1-score for random forest with Hyperopt: {:.3f}'.format(metrics.f1_score(y_train, y_train_pred)))\n",
    "\n",
    "# predict and calculate results for the testing\n",
    "y_test_pred = model.predict(X_test)\n",
    "print('Test F1-score for random forest with Hyperopt: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Part IV. Optuna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_1(trial):\n",
    "    \"\"\"function to iterate over hyperparameters from the first set\n",
    "    \n",
    "    Args:\n",
    "        trial : hyperparameter class\n",
    "      \n",
    "    Returns:\n",
    "        score(float): target metric - F1\n",
    "    \"\"\"\n",
    "    \n",
    "    # space for searching\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'none'])\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sag', 'newton-cg'])\n",
    "    C = trial.suggest_float('C', 0.1, 1)  \n",
    "    \n",
    "    # build a model\n",
    "    model = linear_model.LogisticRegression(\n",
    "        penalty = penalty,\n",
    "        solver = solver,\n",
    "        C=C,\n",
    "        random_state=42,\n",
    "        max_iter=50)   \n",
    "    \n",
    "    # training the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # compete a cross validation\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def optuna_2(trial):\n",
    "    \"\"\"function to iterate over hyperparameters from the second set\n",
    "    \n",
    "    Args:\n",
    "        trial : hyperparameter class\n",
    "      \n",
    "    Returns:\n",
    "        score(float): target metric - F1\n",
    "    \"\"\"\n",
    "    \n",
    "    # space for searching\n",
    "    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "    C = trial.suggest_float('C', 0.1, 1)  \n",
    "    \n",
    "    # build a model\n",
    "    model = linear_model.LogisticRegression(\n",
    "        penalty = penalty,\n",
    "        solver = solver,\n",
    "        C=C,\n",
    "        random_state=42,\n",
    "        max_iter=50)   \n",
    "    \n",
    "    # training the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # compete a cross validation\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-18 15:59:22,742]\u001b[0m A new study created in memory with name: LogisticRegression\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-12-18 15:59:24,396]\u001b[0m Trial 0 finished with value: 0.7574579769635668 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.8445211524878764}. Best is trial 0 with value: 0.7574579769635668.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 15:59:30,764]\u001b[0m Trial 1 finished with value: 0.7789014212164245 and parameters: {'penalty': 'none', 'solver': 'sag', 'C': 0.9931481451194933}. Best is trial 1 with value: 0.7789014212164245.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 15:59:37,529]\u001b[0m Trial 2 finished with value: 0.7797781171199327 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.927990581875355}. Best is trial 2 with value: 0.7797781171199327.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 15:59:49,101]\u001b[0m Trial 3 finished with value: 0.7764793006205497 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.931305520411422}. Best is trial 2 with value: 0.7797781171199327.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 15:59:55,940]\u001b[0m Trial 4 finished with value: 0.7789146448985715 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.7450207782095517}. Best is trial 2 with value: 0.7797781171199327.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:00:05,197]\u001b[0m Trial 5 finished with value: 0.7799605644639646 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.4683495917226359}. Best is trial 5 with value: 0.7799605644639646.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-12-18 16:00:07,043]\u001b[0m Trial 6 finished with value: 0.7574579769635668 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.8353943299641156}. Best is trial 5 with value: 0.7799605644639646.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2022-12-18 16:01:18,361]\u001b[0m Trial 7 finished with value: 0.7127612076011095 and parameters: {'penalty': 'none', 'solver': 'newton-cg', 'C': 0.6402573914438631}. Best is trial 5 with value: 0.7799605644639646.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:01:27,540]\u001b[0m Trial 8 finished with value: 0.7790436255352942 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.38970204615667536}. Best is trial 5 with value: 0.7799605644639646.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:01:38,050]\u001b[0m Trial 9 finished with value: 0.7774604151702466 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.8016542650046907}. Best is trial 5 with value: 0.7799605644639646.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:01:45,853]\u001b[0m Trial 10 finished with value: 0.7836601177047018 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.16234541356114635}. Best is trial 10 with value: 0.7836601177047018.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:01:53,227]\u001b[0m Trial 11 finished with value: 0.7859847557147752 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.10500525902437335}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:00,932]\u001b[0m Trial 12 finished with value: 0.7840860846515072 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.14371161533623938}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:08,064]\u001b[0m Trial 13 finished with value: 0.7856123109552338 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.10215477013615584}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:17,038]\u001b[0m Trial 14 finished with value: 0.7783476436409609 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.27517619290441286}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-12-18 16:02:18,838]\u001b[0m Trial 15 finished with value: 0.7805882062754529 and parameters: {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.30868838691101314}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:27,343]\u001b[0m Trial 16 finished with value: 0.7786633536378664 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.24367370199635202}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:35,245]\u001b[0m Trial 17 finished with value: 0.7837827324743722 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.12601314536993408}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2022-12-18 16:02:37,223]\u001b[0m Trial 18 finished with value: 0.7574579769635668 and parameters: {'penalty': 'none', 'solver': 'lbfgs', 'C': 0.5534870934772363}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:02:43,988]\u001b[0m Trial 19 finished with value: 0.7787193738751728 and parameters: {'penalty': 'l2', 'solver': 'sag', 'C': 0.3861507564546901}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:02:52,534]\u001b[0m Trial 20 finished with value: 0.7792633734347779 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.23470385554272896}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:03:00,627]\u001b[0m Trial 21 finished with value: 0.7840860846515072 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.14381517461984106}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:03:07,749]\u001b[0m Trial 22 finished with value: 0.7859587576528776 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.10021463262548075}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:03:16,347]\u001b[0m Trial 23 finished with value: 0.7807474212956713 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.20943938552259636}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:03:25,959]\u001b[0m Trial 24 finished with value: 0.7790329475724629 and parameters: {'penalty': 'l2', 'solver': 'newton-cg', 'C': 0.34545443532326614}. Best is trial 11 with value: 0.7859847557147752.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score for log reg with Optuna: 0.787\n"
     ]
    }
   ],
   "source": [
    "# finding best parameters\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study_1 = optuna.create_study(study_name=\"LogisticRegression\", direction=\"maximize\")\n",
    "study_1.optimize(optuna_1, n_trials=25)\n",
    "\n",
    "# build a model with best params\n",
    "log_reg_best_1 = linear_model.LogisticRegression(**study_1.best_params,random_state=42)\n",
    "\n",
    "# training the model\n",
    "log_reg_best_1.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the testing sample\n",
    "y_test_pred = log_reg_best_1.predict(X_test)\n",
    "print('Test F1-score for log reg with Optuna: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-18 16:03:28,087]\u001b[0m A new study created in memory with name: LogisticRegression\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:03:30,148]\u001b[0m Trial 0 finished with value: 0.7810893505547788 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.5239690538924806}. Best is trial 0 with value: 0.7810893505547788.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:03:42,539]\u001b[0m Trial 1 finished with value: 0.7833927351874851 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 0.666972270209778}. Best is trial 1 with value: 0.7833927351874851.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:03:53,877]\u001b[0m Trial 2 finished with value: 0.7875943300206008 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 0.4579481018201026}. Best is trial 2 with value: 0.7875943300206008.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:04:05,972]\u001b[0m Trial 3 finished with value: 0.7811816513962073 and parameters: {'penalty': 'l1', 'solver': 'saga', 'C': 0.8243614381116966}. Best is trial 2 with value: 0.7875943300206008.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:08,046]\u001b[0m Trial 4 finished with value: 0.7748985094345315 and parameters: {'penalty': 'l2', 'solver': 'liblinear', 'C': 0.8714188469362244}. Best is trial 2 with value: 0.7875943300206008.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:10,957]\u001b[0m Trial 5 finished with value: 0.786202029080412 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.6689484081393964}. Best is trial 2 with value: 0.7875943300206008.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:12,470]\u001b[0m Trial 6 finished with value: 0.7887725892391461 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.4221536782046321}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:04:20,164]\u001b[0m Trial 7 finished with value: 0.7796443237448003 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 0.9361491096061666}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:04:27,263]\u001b[0m Trial 8 finished with value: 0.7797957079286081 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 0.45126310650222456}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "c:\\Users\\volod\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-12-18 16:04:34,715]\u001b[0m Trial 9 finished with value: 0.7794232631690666 and parameters: {'penalty': 'l2', 'solver': 'saga', 'C': 0.4943223851844911}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:35,682]\u001b[0m Trial 10 finished with value: 0.7880591052981091 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.13396677218775793}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:36,622]\u001b[0m Trial 11 finished with value: 0.7885078462925056 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.12253883899078077}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:37,741]\u001b[0m Trial 12 finished with value: 0.7876742122334707 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.12075070156170525}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:39,054]\u001b[0m Trial 13 finished with value: 0.7877629429734421 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.27909277816610384}. Best is trial 6 with value: 0.7887725892391461.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:40,395]\u001b[0m Trial 14 finished with value: 0.7888480368389512 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.2724883255123107}. Best is trial 14 with value: 0.7888480368389512.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:42,230]\u001b[0m Trial 15 finished with value: 0.7890248612255106 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.3063181680612336}. Best is trial 15 with value: 0.7890248612255106.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:44,662]\u001b[0m Trial 16 finished with value: 0.7876337379629608 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.2773360588903523}. Best is trial 15 with value: 0.7890248612255106.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:46,949]\u001b[0m Trial 17 finished with value: 0.7890248612255106 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.3055135288912561}. Best is trial 15 with value: 0.7890248612255106.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:48,476]\u001b[0m Trial 18 finished with value: 0.787514141408823 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.3657080445661071}. Best is trial 15 with value: 0.7890248612255106.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:51,268]\u001b[0m Trial 19 finished with value: 0.7863213111516284 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.672074482966121}. Best is trial 15 with value: 0.7890248612255106.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:52,399]\u001b[0m Trial 20 finished with value: 0.7892426307818792 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.21742082134343227}. Best is trial 20 with value: 0.7892426307818792.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:53,717]\u001b[0m Trial 21 finished with value: 0.7884582367374218 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.23401149321846057}. Best is trial 20 with value: 0.7892426307818792.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:55,967]\u001b[0m Trial 22 finished with value: 0.7886702082348334 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.35631427740874577}. Best is trial 20 with value: 0.7892426307818792.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:57,146]\u001b[0m Trial 23 finished with value: 0.7904754780685301 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.20193906997814948}. Best is trial 23 with value: 0.7904754780685301.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:04:58,554]\u001b[0m Trial 24 finished with value: 0.790352208511129 and parameters: {'penalty': 'l1', 'solver': 'liblinear', 'C': 0.20792632580227047}. Best is trial 23 with value: 0.7904754780685301.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score for log reg with Optuna: 0.778\n"
     ]
    }
   ],
   "source": [
    "# finding best parameters\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study_2 = optuna.create_study(study_name=\"LogisticRegression\", direction=\"maximize\")\n",
    "study_2.optimize(optuna_2, n_trials=25)\n",
    "\n",
    "# build a model with best params\n",
    "log_reg_best_2 = linear_model.LogisticRegression(**study_2.best_params,random_state=42)\n",
    "\n",
    "# training the model \n",
    "log_reg_best_2.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the testing sample\n",
    "y_test_pred = log_reg_best_2.predict(X_test)\n",
    "print('Test F1-score for log reg with Optuna: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_forest(trial):\n",
    "    \"\"\"function to iterate over hyperparameters for random forest\n",
    "    \n",
    "    Args:\n",
    "        trial : hyperparameter class\n",
    "      \n",
    "    Returns:\n",
    "        score(float): target metric - F1\n",
    "    \"\"\"\n",
    "    \n",
    "    # space for searching\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 250, 25) \n",
    "    max_depth = trial.suggest_int('max_depth', 5, 15, 1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 3, 6, 1)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    # build a model\n",
    "    random_forest = ensemble.RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        random_state=42)   \n",
    "    \n",
    "    # training the model\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    # compete a cross validation\n",
    "    score = cross_val_score(random_forest, X_train, y_train, cv=5, scoring=\"f1\", n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-18 16:04:59,132]\u001b[0m A new study created in memory with name: RandomForest\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:03,747]\u001b[0m Trial 0 finished with value: 0.8083553978762932 and parameters: {'n_estimators': 125, 'max_depth': 14, 'min_samples_leaf': 5, 'criterion': 'gini'}. Best is trial 0 with value: 0.8083553978762932.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:11,685]\u001b[0m Trial 1 finished with value: 0.8051366853195903 and parameters: {'n_estimators': 250, 'max_depth': 9, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 0 with value: 0.8083553978762932.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:20,376]\u001b[0m Trial 2 finished with value: 0.8037422019731902 and parameters: {'n_estimators': 225, 'max_depth': 10, 'min_samples_leaf': 6, 'criterion': 'gini'}. Best is trial 0 with value: 0.8083553978762932.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:25,060]\u001b[0m Trial 3 finished with value: 0.7969365774578533 and parameters: {'n_estimators': 125, 'max_depth': 9, 'min_samples_leaf': 6, 'criterion': 'entropy'}. Best is trial 0 with value: 0.8083553978762932.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:33,037]\u001b[0m Trial 4 finished with value: 0.8110710360417779 and parameters: {'n_estimators': 200, 'max_depth': 12, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:38,136]\u001b[0m Trial 5 finished with value: 0.8086042901402868 and parameters: {'n_estimators': 150, 'max_depth': 12, 'min_samples_leaf': 6, 'criterion': 'gini'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:43,063]\u001b[0m Trial 6 finished with value: 0.7770899802624605 and parameters: {'n_estimators': 200, 'max_depth': 6, 'min_samples_leaf': 5, 'criterion': 'entropy'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:45,378]\u001b[0m Trial 7 finished with value: 0.7647544995912254 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:49,828]\u001b[0m Trial 8 finished with value: 0.7703213576814394 and parameters: {'n_estimators': 250, 'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:51,883]\u001b[0m Trial 9 finished with value: 0.7629668963224115 and parameters: {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 6, 'criterion': 'gini'}. Best is trial 4 with value: 0.8110710360417779.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:05:59,458]\u001b[0m Trial 10 finished with value: 0.8113368609383965 and parameters: {'n_estimators': 175, 'max_depth': 15, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 10 with value: 0.8113368609383965.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:06,989]\u001b[0m Trial 11 finished with value: 0.8113368609383965 and parameters: {'n_estimators': 175, 'max_depth': 15, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 10 with value: 0.8113368609383965.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:14,426]\u001b[0m Trial 12 finished with value: 0.8113368609383965 and parameters: {'n_estimators': 175, 'max_depth': 15, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 10 with value: 0.8113368609383965.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:21,515]\u001b[0m Trial 13 finished with value: 0.8113363492558715 and parameters: {'n_estimators': 175, 'max_depth': 13, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 10 with value: 0.8113368609383965.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:28,055]\u001b[0m Trial 14 finished with value: 0.8117761350388817 and parameters: {'n_estimators': 150, 'max_depth': 15, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 14 with value: 0.8117761350388817.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:34,320]\u001b[0m Trial 15 finished with value: 0.8122258288615155 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 15 with value: 0.8122258288615155.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:40,398]\u001b[0m Trial 16 finished with value: 0.8122258288615155 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 15 with value: 0.8122258288615155.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:46,583]\u001b[0m Trial 17 finished with value: 0.807933403146279 and parameters: {'n_estimators': 150, 'max_depth': 12, 'min_samples_leaf': 3, 'criterion': 'entropy'}. Best is trial 15 with value: 0.8122258288615155.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:51,080]\u001b[0m Trial 18 finished with value: 0.8027696241338329 and parameters: {'n_estimators': 125, 'max_depth': 10, 'min_samples_leaf': 5, 'criterion': 'entropy'}. Best is trial 15 with value: 0.8122258288615155.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:06:56,921]\u001b[0m Trial 19 finished with value: 0.8165244441698963 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:07:04,006]\u001b[0m Trial 20 finished with value: 0.8129477287098489 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:07:10,991]\u001b[0m Trial 21 finished with value: 0.8129477287098489 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:07:18,069]\u001b[0m Trial 22 finished with value: 0.8129477287098489 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:07:26,207]\u001b[0m Trial 23 finished with value: 0.8134867150456161 and parameters: {'n_estimators': 225, 'max_depth': 11, 'min_samples_leaf': 4, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n",
      "\u001b[32m[I 2022-12-18 16:07:32,485]\u001b[0m Trial 24 finished with value: 0.7937838085451763 and parameters: {'n_estimators': 225, 'max_depth': 8, 'min_samples_leaf': 5, 'criterion': 'entropy'}. Best is trial 19 with value: 0.8165244441698963.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score for random forest with Optuna: 0.792\n"
     ]
    }
   ],
   "source": [
    "# finding best parameters\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study_forest = optuna.create_study(study_name=\"RandomForest\", direction=\"maximize\")\n",
    "study_forest.optimize(optuna_forest, n_trials=25)\n",
    "\n",
    "# build a model with best parameters\n",
    "random_forest_best = ensemble.RandomForestClassifier(**study_forest.best_params, random_state=42)\n",
    "\n",
    "# training the model\n",
    "random_forest_best.fit(X_train, y_train)\n",
    "\n",
    "# predict and calculate results for the testing sample\n",
    "y_test_pred = random_forest_best.predict(X_test)\n",
    "print('Test F1-score for random forest with Optuna: {:.3f}'.format(metrics.f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <span style=\"color: red\"> **Conclusion**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warnings!** The target metric maximization is not the purpose of this assignment! The main goal is using various options to optimize hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of basic optimization models such as *GridSearchCV* and *RandomizedSearchCV* shows high results, but is an extremely time-consuming and resource-consuming optimization method.  \n",
    "In this regard, using advanced optimization looks much more practical. **Optuna** and **Hyperopt** increase the metric even for a small selection of parameters, while taking much less time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1-score for logistic regression model:**\n",
    "- Baseline: 0.777\n",
    "- GridSearchCV: 0.781\n",
    "- RandomizedSearchCV: 0.781\n",
    "- Hyperopt: 0.784\n",
    "- Optuna: **0.787**\n",
    "\n",
    "**F1-score for logistic regression model:**\n",
    "- Baseline: 0.737\n",
    "- GridSearchCV: 0.799\n",
    "- RandomizedSearchCV: 0.80\n",
    "- Hyperopt: **0.803**\n",
    "- Optuna: 0.792"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d181b88a5681332783464b335ffcae2a192662c591f914e6515f1b7dbb1ef53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
